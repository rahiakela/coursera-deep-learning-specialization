{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week-3-neural_machine_translation_with_attention.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMBkzyR7UnYAUW6sN9rPO+A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/coursera-deep-learning-specialization/blob/course-5-sequence-models-recurrent-neural-networks/week_3_neural_machine_translation_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhSS9NFZhKDx",
        "colab_type": "text"
      },
      "source": [
        "# Neural Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1dR6avMhKth",
        "colab_type": "text"
      },
      "source": [
        "Welcome to your first programming assignment for this week! \n",
        "\n",
        "* You will build a Neural Machine Translation (NMT) model to translate human-readable dates (\"25th of June, 2009\") into machine-readable dates (\"2009-06-25\"). \n",
        "* You will do this using an attention model, one of the most sophisticated sequence-to-sequence models. \n",
        "\n",
        "This notebook was produced together with NVIDIA's Deep Learning Institute. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RodYE-c8haSE",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Let's load all the packages you will need for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtH2_QKIksxx",
        "colab_type": "code",
        "outputId": "3068012a-61a5-45f5-aa1b-fba9ad8c765d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "! pip install faker"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting faker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/fa/cc588102bbc75983141265d0013dd42cb4e6223961ee03f1bdbcf6e9d4e2/Faker-4.0.0-py3-none-any.whl (963kB)\n",
            "\u001b[K     |████████████████████████████████| 972kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.6/dist-packages (from faker) (1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.6/dist-packages (from faker) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.4->faker) (1.12.0)\n",
            "Installing collected packages: faker\n",
            "Successfully installed faker-4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw6DEhN7hcIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "from nmt_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIIDXo_KhmES",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Translating human readable dates into machine readable dates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Im9wYNCiUMu",
        "colab_type": "text"
      },
      "source": [
        "* The model you will build here could be used to translate from one language to another, such as translating from English to Hindi. \n",
        "* However, language translation requires massive datasets and usually takes days of training on GPUs. \n",
        "* To give you a place to experiment with these models without using massive datasets, we will perform a simpler \"date translation\" task. \n",
        "* The network will input a date written in a variety of possible formats (*e.g. \"the 29th of August 1958\", \"03/30/1968\", \"24 JUNE 1987\"*) \n",
        "* The network will translate them into standardized, machine readable dates (*e.g. \"1958-08-29\", \"1968-03-30\", \"1987-06-24\"*). \n",
        "* We will have the network learn to output dates in the common machine-readable format YYYY-MM-DD. \n",
        "\n",
        "<!-- \n",
        "Take a look at [nmt_utils.py](./nmt_utils.py) to see all the formatting. Count and figure out how the formats work, you will need this knowledge later. !--> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPgPv8RjiZP5",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 - Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxml2YmxiZ4S",
        "colab_type": "text"
      },
      "source": [
        "We will train the model on a dataset of 10,000 human readable dates and their equivalent, standardized, machine readable dates. Let's run the following cells to load the dataset and print some examples. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxal0VQ2ib5F",
        "colab_type": "code",
        "outputId": "21fc3763-7e6d-433a-c904-e047920dccd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "m = 10000\n",
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 19391.15it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OvHaXdsiqRp",
        "colab_type": "code",
        "outputId": "c4e41748-c5b8-44d2-e3e3-c680f9bca174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "dataset[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('9 may 1998', '1998-05-09'),\n",
              " ('10.11.19', '2019-11-10'),\n",
              " ('9/10/70', '1970-09-10'),\n",
              " ('saturday april 28 1990', '1990-04-28'),\n",
              " ('thursday january 26 1995', '1995-01-26'),\n",
              " ('monday march 7 1983', '1983-03-07'),\n",
              " ('sunday may 22 1988', '1988-05-22'),\n",
              " ('08 jul 2008', '2008-07-08'),\n",
              " ('8 sep 1999', '1999-09-08'),\n",
              " ('thursday january 1 1981', '1981-01-01')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZzGYaszjh3A",
        "colab_type": "text"
      },
      "source": [
        "You've loaded:\n",
        "- `dataset`: a list of tuples of (human readable date, machine readable date).\n",
        "- `human_vocab`: a python dictionary mapping all characters used in the human readable dates to an integer-valued index.\n",
        "- `machine_vocab`: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. \n",
        "    - **Note**: These indices are not necessarily consistent with `human_vocab`. \n",
        "- `inv_machine_vocab`: the inverse dictionary of `machine_vocab`, mapping from indices back to characters. \n",
        "\n",
        "Let's preprocess the data and map the raw text data into the index values. \n",
        "- We will set Tx=30 \n",
        "    - We assume Tx is the maximum length of the human readable date.\n",
        "    - If we get a longer input, we would have to truncate it.\n",
        "- We will set Ty=10\n",
        "    - \"YYYY-MM-DD\" is 10 characters long."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tJM6481jisd",
        "colab_type": "code",
        "outputId": "be2b5544-9e49-4b64-8931-a26e2d5ffff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "Tx = 30\n",
        "Ty = 10\n",
        "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
        "\n",
        "print(f'X.shape: {X.shape}')\n",
        "print(f'Y.shape: {Y.shape}')\n",
        "print(f'Xoh.shape: {Xoh.shape}')\n",
        "print(f'Yoh.shape: {Yoh.shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape: (10000, 30)\n",
            "Y.shape: (10000, 10)\n",
            "Xoh.shape: (10000, 30, 37)\n",
            "Yoh.shape: (10000, 10, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO-8TN1-j-Wp",
        "colab_type": "text"
      },
      "source": [
        "You now have:\n",
        "- `X`: a processed version of the human readable dates in the training set.\n",
        "    - Each character in X is replaced by an index (integer) mapped to the character using `human_vocab`. \n",
        "    - Each date is padded to ensure a length of $T_x$ using a special character (< pad >). \n",
        "    - `X.shape = (m, Tx)` where m is the number of training examples in a batch.\n",
        "- `Y`: a processed version of the machine readable dates in the training set.\n",
        "    - Each character is replaced by the index (integer) it is mapped to in `machine_vocab`. \n",
        "    - `Y.shape = (m, Ty)`. \n",
        "- `Xoh`: one-hot version of `X`\n",
        "    - Each index in `X` is converted to the one-hot representation (if the index is 2, the one-hot version has the index position 2 set to 1, and the remaining positions are 0.\n",
        "    - `Xoh.shape = (m, Tx, len(human_vocab))`\n",
        "- `Yoh`: one-hot version of `Y`\n",
        "    - Each index in `Y` is converted to the one-hot representation. \n",
        "    - `Yoh.shape = (m, Tx, len(machine_vocab))`. \n",
        "    - `len(machine_vocab) = 11` since there are 10 numeric digits (0 to 9) and the `-` symbol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSBD29vdl2ID",
        "colab_type": "code",
        "outputId": "cdbac679-8c8c-48bc-f5ab-f1213d6da3d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "index = 5\n",
        "print(f'Source date: {dataset[index][0]}')\n",
        "print(f'Target date: {dataset[index][1]}')\n",
        "print()\n",
        "\n",
        "print(f'Source after preprocessing (indices): {X[index]}')\n",
        "print(f'Target after preprocessing (indices): {Y[index]}')\n",
        "print()\n",
        "\n",
        "print(f'Source after preprocessing (one-hot): {Xoh[index]}')\n",
        "print(f'Target after preprocessing (one-hot): {Yoh[index]}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source date: monday march 7 1983\n",
            "Target date: 1983-03-07\n",
            "\n",
            "Source after preprocessing (indices): [24 26 25 16 13 34  0 24 13 28 15 20  0 10  0  4 12 11  6 36 36 36 36 36\n",
            " 36 36 36 36 36 36]\n",
            "Target after preprocessing (indices): [ 2 10  9  4  0  1  4  0  1  8]\n",
            "\n",
            "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5_qyaQdm03W",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Neural machine translation with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leoj8oa2m1bY",
        "colab_type": "text"
      },
      "source": [
        "* If you had to translate a book's paragraph from French to English, you would not read the whole paragraph, then close the book and translate. \n",
        "* Even during the translation process, you would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English you are writing down. \n",
        "* The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqSVqGpsm3cs",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 - Attention mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdqkKuM0m5T_",
        "colab_type": "text"
      },
      "source": [
        "In this part, you will implement the attention mechanism presented in the lecture videos. \n",
        "* Here is a figure to remind you how the model works. \n",
        "    * The diagram on the left shows the attention model. \n",
        "    * The diagram on the right shows what one \"attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$.\n",
        "    * The attention variables $\\alpha^{\\langle t, t' \\rangle}$ are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$). \n",
        "\n",
        "<table>\n",
        "<td> \n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-specialization/attn_model.png?raw=1' width='800'/><br>\n",
        "</td> \n",
        "<td> \n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-specialization/attn_mechanism.png?raw=1' width='800'/>\n",
        "<br>\n",
        "</td> \n",
        "</table>\n",
        "<caption><center> **Figure 1**: Neural machine translation with attention</center></caption>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA_3xKulmieU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}