{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week-2-operations_on_word_vectors.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOvl7lTgrbSuYDQk4x4I+PE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/coursera-deep-learning-specialization/blob/course-5-sequence-models-recurrent-neural-networks/week_2_operations_on_word_vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13sN0tN8OeCH",
        "colab_type": "text"
      },
      "source": [
        "# Operations on word vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdbNxPH3OenD",
        "colab_type": "text"
      },
      "source": [
        "Welcome to your first assignment of this week! \n",
        "\n",
        "Because word embeddings are very computationally expensive to train, most ML practitioners will load a pre-trained set of embeddings. \n",
        "\n",
        "**After this assignment you will be able to:**\n",
        "\n",
        "- Load pre-trained word vectors, and measure similarity using cosine similarity\n",
        "- Use word embeddings to solve word analogy problems such as Man is to Woman as King is to ______. \n",
        "- Modify word embeddings to reduce their gender bias "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij67akqBOnRL",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Let's get started! Run the following cell to load the packages you will need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3-oDPnTOpB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "81228228-cbdf-4f2f-8933-e5e2bf462602"
      },
      "source": [
        "import numpy as np\n",
        "from w2v_utils import *"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCM7XyVcSK1c",
        "colab_type": "text"
      },
      "source": [
        "#### Load the word vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqCUrj-YSLac",
        "colab_type": "text"
      },
      "source": [
        "* For this assignment, we will use 50-dimensional GloVe vectors to represent words. \n",
        "* Run the following cell to load the `word_to_vec_map`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRhje40BSOTb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "0e0541f9-8532-4b09-e351-48cce7b8717e"
      },
      "source": [
        "words, word_to_vec_map = read_glove_vecs('../data/glove.6B.50d.txt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e327b17382dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_to_vec_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_glove_vecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/glove.6B.50d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/w2v_utils.py\u001b[0m in \u001b[0;36mread_glove_vecs\u001b[0;34m(glove_file)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_glove_vecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mword_to_vec_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/glove.6B.50d.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx9cKSm1TrXv",
        "colab_type": "text"
      },
      "source": [
        "You've loaded:\n",
        "- `words`: set of words in the vocabulary.\n",
        "- `word_to_vec_map`: dictionary mapping words to their GloVe vector representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YSFMCd4TvIH",
        "colab_type": "text"
      },
      "source": [
        "#### Embedding vectors versus one-hot vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrJcKS2VTw_K",
        "colab_type": "text"
      },
      "source": [
        "* Recall from the lesson videos that one-hot vectors do not do a good job of capturing the level of similarity between words (every one-hot vector has the same Euclidean distance from any other one-hot vector).\n",
        "* Embedding vectors such as GloVe vectors provide much more useful information about the meaning of individual words. \n",
        "* Lets now see how you can use GloVe vectors to measure the similarity between two words. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMMxGE3sTzQv",
        "colab_type": "text"
      },
      "source": [
        "# 1 - Cosine similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_67pRmrUZAv",
        "colab_type": "text"
      },
      "source": [
        "To measure the similarity between two words, we need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: \n",
        "\n",
        "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2} = cos(\\theta)Â \\tag{1}$$\n",
        "\n",
        "* $u \\cdot v$ is the dot product (or inner product) of two vectors\n",
        "* $||u||_2$ is the norm (or length) of the vector $u$\n",
        "* $\\theta$ is the angle between $u$ and $v$. \n",
        "* The cosine similarity depends on the angle between $u$ and $v$. \n",
        "    * If $u$ and $v$ are very similar, their cosine similarity will be close to 1.\n",
        "    * If they are dissimilar, the cosine similarity will take a smaller value. \n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-specialization/cosine_sim.png?raw=1' width='800'/>\n",
        "<caption><center> **Figure 1**: The cosine of the angle between two vectors is a measure their similarity</center></caption>\n",
        "\n",
        "**Exercise**: Implement the function `cosine_similarity()` to evaluate the similarity between word vectors.\n",
        "\n",
        "**Reminder**: The norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$\n",
        "\n",
        "#### Additional Hints\n",
        "* You may find `np.dot`, `np.sum`, or `np.sqrt` useful depending upon the implementation that you choose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY8BAnO6Uc6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCTION: cosine_similarity\n",
        "\n",
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    Cosine similarity reflects the degree of similarity between u and v\n",
        "        \n",
        "    Arguments:\n",
        "        u -- a word vector of shape (n,)          \n",
        "        v -- a word vector of shape (n,)\n",
        "\n",
        "    Returns:\n",
        "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
        "    \"\"\"\n",
        "    \n",
        "    distance = 0.0\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Compute the dot product between u and v (â1 line)\n",
        "    dot = np.dot(u, v)\n",
        "    # Compute the L2 norm of u (â1 line)\n",
        "    norm_u = np.sqrt(np.sum(u ** 2))\n",
        "    \n",
        "    # Compute the L2 norm of v (â1 line)\n",
        "    norm_v = np.sqrt(np.sum(v ** 2))\n",
        "    # Compute the cosine similarity defined by formula (1) (â1 line)\n",
        "    cosine_similarity = dot / np.dot(norm_u, norm_v)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_Nn-MtPXbqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "father = word_to_vec_map['father']\n",
        "mother = word_to_vec_map['mother']\n",
        "ball = word_to_vec_map['ball']\n",
        "crocodile = word_to_vec_map['crocodile']\n",
        "france = word_to_vec_map['france']\n",
        "italy = word_to_vec_map['italy']\n",
        "paris = word_to_vec_map['paris']\n",
        "rome = word_to_vec_map['rome']\n",
        "\n",
        "print(f'cosine_similarity(father, mother) = {cosine_similarity(father, mother)}')\n",
        "print(f'cosine_similarity(ball, mother) = {cosine_similarity(ball, crocodile)}')\n",
        "print(f'cosine_similarity(france - paris, rome - italy) = {cosine_similarity(france - paris, rome - italy)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aokICnFuc88n",
        "colab_type": "text"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **cosine_similarity(father, mother)** =\n",
        "        </td>\n",
        "        <td>\n",
        "         0.890903844289\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **cosine_similarity(ball, crocodile)** =\n",
        "        </td>\n",
        "        <td>\n",
        "         0.274392462614\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **cosine_similarity(france - paris, rome - italy)** =\n",
        "        </td>\n",
        "        <td>\n",
        "         -0.675147930817\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86PNxq8udCKV",
        "colab_type": "text"
      },
      "source": [
        "#### Try different words!\n",
        "* After you get the correct expected output, please feel free to modify the inputs and measure the cosine similarity between other pairs of words! \n",
        "* Playing around with the cosine similarity of other inputs will give you a better sense of how word vectors behave."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb4kGiLVdCwm",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Word analogy task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2fOBqTSdFqD",
        "colab_type": "text"
      },
      "source": [
        "* In the word analogy task, we complete the sentence:  \n",
        "    <font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>. \n",
        "\n",
        "* An example is:  \n",
        "    <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>. \n",
        "\n",
        "* We are trying to find a word *d*, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner:   \n",
        "    $e_b - e_a \\approx e_d - e_c$\n",
        "* We will measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity. \n",
        "\n",
        "**Exercise**: Complete the code below to be able to perform word analogies!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79ItyN2ldJGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCTION: complete_analogy\n",
        "\n",
        "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Performs the word analogy task as explained above: a is to b as c is to ____. \n",
        "    \n",
        "    Arguments:\n",
        "    word_a -- a word, string\n",
        "    word_b -- a word, string\n",
        "    word_c -- a word, string\n",
        "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
        "    \n",
        "    Returns:\n",
        "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
        "    \"\"\"\n",
        "    \n",
        "    # convert words to lowercase\n",
        "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Get the word embeddings e_a, e_b and e_c (â1-3 lines)\n",
        "    e_a, e_b, e_c = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    words = word_to_vec_map.keys()\n",
        "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
        "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
        "\n",
        "    # to avoid best_word being one of the input words, skip the input words\n",
        "    # place the input words in a set for faster searching than a list\n",
        "    # We will re-use this set of input words inside the for-loop\n",
        "    input_words_set = set([word_a, word_b, word_c])\n",
        "    \n",
        "    # loop over the whole word vector set\n",
        "    for w in words:        \n",
        "        # to avoid best_word being one of the input words, skip the input words\n",
        "        if w in input_words_set:\n",
        "            continue\n",
        "        \n",
        "        ### START CODE HERE ###\n",
        "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (â1 line)\n",
        "        cosine_sim = None\n",
        "        \n",
        "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
        "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (â3 lines)\n",
        "        if None > None:\n",
        "            max_cosine_sim = None\n",
        "            best_word = None\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    return best_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-1tbDIbdQ7E",
        "colab_type": "text"
      },
      "source": [
        "Run the cell below to test your code, this may take 1-2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oOjmxT9dRau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "triads_to_try = [\n",
        "    ('italy', 'italian', 'spain'),\n",
        "    ('india', 'delhi', 'japan'),\n",
        "    ('man', 'woman', 'boy'),\n",
        "    ('small', 'smaller', 'large')        \n",
        "]\n",
        "for triad in triads_to_try:\n",
        "  print('{} -> {} :: {} -> {}'.format(*triad, complete_analogy(*triad, word_to_vec_map)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgMULgSLef9m",
        "colab_type": "text"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **italy -> italian** ::\n",
        "        </td>\n",
        "        <td>\n",
        "         spain -> spanish\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **india -> delhi** ::\n",
        "        </td>\n",
        "        <td>\n",
        "         japan -> tokyo\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **man -> woman ** ::\n",
        "        </td>\n",
        "        <td>\n",
        "         boy -> girl\n",
        "        </td>\n",
        "    </tr>\n",
        "        <tr>\n",
        "        <td>\n",
        "            **small -> smaller ** ::\n",
        "        </td>\n",
        "        <td>\n",
        "         large -> larger\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGq0noaeelHN",
        "colab_type": "text"
      },
      "source": [
        "* Once you get the correct expected output, please feel free to modify the input cells above to test your own analogies. \n",
        "* Try to find some other analogy pairs that do work, but also find some where the algorithm doesn't give the right answer:\n",
        "    * For example, you can try small->smaller as big->?."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-6D1gqKelnz",
        "colab_type": "text"
      },
      "source": [
        "### Congratulations!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THKf38MkepHC",
        "colab_type": "text"
      },
      "source": [
        "You've come to the end of the graded portion of the assignment. Here are the main points you should remember:\n",
        "\n",
        "- Cosine similarity is a good way to compare the similarity between pairs of word vectors.\n",
        "    - Note that L2 (Euclidean) distance also works.\n",
        "- For NLP applications, using a pre-trained set of word vectors is often a good way to get started.\n",
        "- Even though you have finished the graded portions, we recommend you take a look at the rest of this notebook to learn about debiasing word vectors.\n",
        "\n",
        "Congratulations on finishing the graded portions of this notebook! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X-vYGHFerBw",
        "colab_type": "text"
      },
      "source": [
        "## 3 - Debiasing word vectors (OPTIONAL/UNGRADED) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqYA06oSetsi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}