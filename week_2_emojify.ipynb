{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week-2-emojify.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPVNCB3kE5YaaIN+2IiVBvs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/coursera-deep-learning-specialization/blob/course-5-sequence-models-recurrent-neural-networks/week_2_emojify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo5niL_U3WfY",
        "colab_type": "text"
      },
      "source": [
        "# Emojify! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSJD9t5L3XEK",
        "colab_type": "text"
      },
      "source": [
        "Welcome to the second assignment of Week 2. You are going to use word vector representations to build an Emojifier. \n",
        "\n",
        "Have you ever wanted to make your text messages more expressive? Your emojifier app will help you do that. \n",
        "So rather than writing:\n",
        ">\"Congratulations on the promotion! Let's get coffee and talk. Love you!\"   \n",
        "\n",
        "The emojifier can automatically turn this into:\n",
        ">\"Congratulations on the promotion! üëç Let's get coffee and talk. ‚òïÔ∏è Love you! ‚ù§Ô∏è\"\n",
        "\n",
        "* You will implement a model which inputs a sentence (such as \"Let's go see the baseball game tonight!\") and finds the most appropriate emoji to be used with this sentence (‚öæÔ∏è)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFbSDn9R3Ze8",
        "colab_type": "text"
      },
      "source": [
        "#### Using word vectors to improve emoji lookups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVyfR65p3d71",
        "colab_type": "text"
      },
      "source": [
        "* In many emoji interfaces, you need to remember that ‚ù§Ô∏è is the \"heart\" symbol rather than the \"love\" symbol. \n",
        "    * In other words, you'll have to remember to type \"heart\" to find the desired emoji, and typing \"love\" won't bring up that symbol.\n",
        "* We can make a more flexible emoji interface by using word vectors!\n",
        "* When using word vectors, you'll see that even if your training set explicitly relates only a few words to a particular emoji, your algorithm will be able to generalize and associate additional words in the test set to the same emoji.\n",
        "    * This works even if those additional words don't even appear in the training set. \n",
        "    * This allows you to build an accurate classifier mapping from sentences to emojis, even using a small training set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQaxbQz33gR7",
        "colab_type": "text"
      },
      "source": [
        "#### What you'll build\n",
        "1. In this exercise, you'll start with a baseline model (Emojifier-V1) using word embeddings.\n",
        "2. Then you will build a more sophisticated model (Emojifier-V2) that further incorporates an LSTM. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA1xhgbe3olP",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Let's get started! Run the following cell to load the package you are going to use. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K19aKdW57H-a",
        "colab_type": "code",
        "outputId": "e18a4221-2b2a-4529-e320-42b4a14f63d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "# install emoji\n",
        "! pip install emoji"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 20kB 3.1MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 30kB 4.0MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51kB 2.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42175 sha256=487efc44aecf1a7c984c0fecb8d4fc7fdfeb14ab62b3e38bf9241347a7edb686\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvSJHtch3p6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from emo_utils import *\n",
        "import emoji\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrAZi4_l7ZfG",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Baseline model: Emojifier-V1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egVgFFvS7aGb",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 - Dataset EMOJISET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXDGZM5S7eN1",
        "colab_type": "text"
      },
      "source": [
        "Let's start by building a simple baseline classifier. \n",
        "\n",
        "You have a tiny dataset (X, Y) where:\n",
        "- X contains 127 sentences (strings).\n",
        "- Y contains an integer label between 0 and 4 corresponding to an emoji for each sentence.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-specialization/data_set.png?raw=1' width='800'/>\n",
        "<caption><center> **Figure 1**: EMOJISET - a classification problem with 5 classes. A few examples of sentences are given here. </center></caption>\n",
        "\n",
        "Let's load the dataset using the code below. We split the dataset between training (127 examples) and testing (56 examples)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWE3WaBA7EL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
        "X_test, Y_test = read_csv('data/tesss.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ER1T-tAHUdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxLen = len(max(X_train, key=len).split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0e2q0OrHX-F",
        "colab_type": "text"
      },
      "source": [
        "Run the following cell to print sentences from X_train and corresponding labels from Y_train. \n",
        "* Change `idx` to see different examples. \n",
        "* Note that due to the font used by iPython notebook, the heart emoji may be colored black rather than red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXRZFJS2HYhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for idx in range(10):\n",
        "    print(X_train[idx], label_to_emoji(Y_train[idx]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPyFoSKFHbhd",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 - Overview of the Emojifier-V1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZBkmWEbHheT",
        "colab_type": "text"
      },
      "source": [
        "In this part, you are going to implement a baseline model called \"Emojifier-v1\".  \n",
        "\n",
        "<center>\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-specialization/image_1.png?raw=1' width='800'/>\n",
        "<caption><center> **Figure 2**: Baseline model (Emojifier-V1).</center></caption>\n",
        "</center>\n",
        "\n",
        "\n",
        "#### Inputs and outputs\n",
        "* The input of the model is a string corresponding to a sentence (e.g. \"I love you). \n",
        "* The output will be a probability vector of shape (1,5), (there are 5 emojis to choose from).\n",
        "* The (1,5) probability vector is passed to an argmax layer, which extracts the index of the emoji with the highest probability.\n",
        "\n",
        "#### One-hot encoding\n",
        "* To get our labels into a format suitable for training a softmax classifier, lets convert $Y$ from its current shape  $(m, 1)$ into a \"one-hot representation\" $(m, 5)$, \n",
        "    * Each row is a one-hot vector giving the label of one example.\n",
        "    * Here, `Y_oh` stands for \"Y-one-hot\" in the variable names `Y_oh_train` and `Y_oh_test`: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwk46GjlHw75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
        "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g49WZSeKH5xX",
        "colab_type": "text"
      },
      "source": [
        "Let's see what `convert_to_one_hot()` did. Feel free to change `index` to print out different values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-NyEDgQH76X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx = 50\n",
        "print(f'Sentence \"{X_train[50]}\" has label index {Y_train[idx]}, which is emoji {label_to_emoji(Y_train[idx])}')\n",
        "print(f'Label index {Y_train[idx]} in one-hot encoding format is {Y_oh_train[idx]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UKOVQOeIPgL",
        "colab_type": "text"
      },
      "source": [
        "All the data is now ready to be fed into the Emojify-V1 model. Let's implement the model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2HddezLIQJu",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 - Implementing Emojifier-V1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssCzHAXAITnW",
        "colab_type": "text"
      },
      "source": [
        "As shown in Figure 2 (above), the first step is to:\n",
        "* Convert each word in the input sentence into their word vector representations.\n",
        "* Then take an average of the word vectors. \n",
        "* Similar to the previous exercise, we will use pre-trained 50-dimensional GloVe embeddings. \n",
        "\n",
        "Run the following cell to load the `word_to_vec_map`, which contains all the vector representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ch0tjHrfIVpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../../readonly/glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovhXLcqxQUZ2",
        "colab_type": "text"
      },
      "source": [
        "You've loaded:\n",
        "- `word_to_index`: dictionary mapping from words to their indices in the vocabulary \n",
        "    - (400,001 words, with the valid indices ranging from 0 to 400,000)\n",
        "- `index_to_word`: dictionary mapping from indices to their corresponding words in the vocabulary\n",
        "- `word_to_vec_map`: dictionary mapping words to their GloVe vector representation.\n",
        "\n",
        "Run the following cell to check if it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9USrjfVIQX0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = \"cucumber\"\n",
        "idx = 289846\n",
        "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
        "print(\"the\", str(idx) + \"th word in the vocabulary is\", index_to_word[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wa5hH8PQa46",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**: Implement `sentence_to_avg()`. You will need to carry out two steps:\n",
        "1. Convert every sentence to lower-case, then split the sentence into a list of words. \n",
        "    * `X.lower()` and `X.split()` might be useful. \n",
        "2. For each word in the sentence, access its GloVe representation.\n",
        "    * Then take the average of all of these word vectors.\n",
        "    * You might use `numpy.zeros()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8s8LzAuQeGw",
        "colab_type": "text"
      },
      "source": [
        "#### Additional Hints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF81ClV2QgAm",
        "colab_type": "text"
      },
      "source": [
        "* When creating the `avg` array of zeros, you'll want it to be a vector of the same shape as the other word vectors in the `word_to_vec_map`.  \n",
        "    * You can choose a word that exists in the `word_to_vec_map` and access its `.shape` field.\n",
        "    * Be careful not to hard code the word that you access.  In other words, don't assume that if you see the word 'the' in the `word_to_vec_map` within this notebook, that this word will be in the `word_to_vec_map` when the function is being called by the automatic grader.\n",
        "    * Hint: you can use any one of the word vectors that you retrieved from the input `sentence` to find the shape of a word vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wakC5L4iQisd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: sentence_to_avg\n",
        "\n",
        "def sentence_to_avg(sentence, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word\n",
        "    and averages its value into a single vector encoding the meaning of the sentence.\n",
        "    \n",
        "    Arguments:\n",
        "    sentence -- string, one training example from X\n",
        "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    \n",
        "    Returns:\n",
        "    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Step 1: Split sentence into list of lower case words (‚âà 1 line)\n",
        "    words = sentence.lower().split()\n",
        "\n",
        "    # Initialize the average word vector, should have the same shape as your word vectors.\n",
        "    avg = np.zeros((50, ))\n",
        "    \n",
        "    # Step 2: average the word vectors. You can loop over the words in the list \"words\".\n",
        "    total = 0\n",
        "    for w in words:\n",
        "        total += word_to_vec_map[w]\n",
        "    avg = total / len(words)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okutMf-TQlQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
        "print(\"avg = \\n\", avg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7NfENrfQnKF",
        "colab_type": "text"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "```Python\n",
        "avg =\n",
        "[-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
        " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
        "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
        "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
        "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
        "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
        " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
        " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
        "  0.1445417   0.09808667]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx0CXluFVmuC",
        "colab_type": "text"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojT-RA60WJvJ",
        "colab_type": "text"
      },
      "source": [
        "You now have all the pieces to finish implementing the `model()` function. \n",
        "After using `sentence_to_avg()` you need to:\n",
        "* Pass the average through forward propagation\n",
        "* Compute the cost\n",
        "* Backpropagate to update the softmax parameters\n",
        "\n",
        "**Exercise**: Implement the `model()` function described in Figure (2). \n",
        "\n",
        "* The equations you need to implement in the forward pass and to compute the cross-entropy cost are below:\n",
        "* The variable $Y_{oh}$ (\"Y one hot\") is the one-hot encoding of the output labels. \n",
        "\n",
        "$$ z^{(i)} = W . avg^{(i)} + b$$\n",
        "\n",
        "$$ a^{(i)} = softmax(z^{(i)})$$\n",
        "\n",
        "$$ \\mathcal{L}^{(i)} = - \\sum_{k = 0}^{n_y - 1} Y_{oh,k}^{(i)} * log(a^{(i)}_k)$$\n",
        "\n",
        "**Note** It is possible to come up with a more efficient vectorized implementation. For now, let's use nested for loops to better understand the algorithm, and for easier debugging.\n",
        "\n",
        "We provided the function `softmax()`, which was imported earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RuDn0FZWMkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
        "    \"\"\"\n",
        "    Model to train word vector representations in numpy.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, numpy array of sentences as strings, of shape (m, 1)\n",
        "    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
        "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
        "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
        "    num_iterations -- number of iterations\n",
        "    \n",
        "    Returns:\n",
        "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
        "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
        "    b -- bias of the softmax layer, of shape (n_y,)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "\n",
        "    # Define number of training examples\n",
        "    m = Y.shape[0]                          # number of training examples\n",
        "    n_y = 5                                 # number of classes  \n",
        "    n_h = 50                                # dimensions of the GloVe vectors \n",
        "    \n",
        "    # Initialize parameters using Xavier initialization\n",
        "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
        "    b = np.zeros((n_y,))\n",
        "    \n",
        "    # Convert Y to Y_onehot with n_y classes\n",
        "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
        "    \n",
        "    # Optimization loop\n",
        "    for t in range(num_iterations): # Loop over the number of iterations\n",
        "        for i in range(m):          # Loop over the training examples\n",
        "            \n",
        "            ### START CODE HERE ### (‚âà 4 lines of code)\n",
        "            # Average the word vectors of the words from the i'th training example\n",
        "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
        "\n",
        "            # Forward propagate the avg through the softmax layer\n",
        "            z = np.dot(W, avg) + b\n",
        "            a = softmax(z)\n",
        "\n",
        "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
        "            cost = - np.sum(Y_oh[i] * np.log(a))\n",
        "            ### END CODE HERE ###\n",
        "            \n",
        "            # Compute gradients \n",
        "            dz = a - Y_oh[i]\n",
        "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
        "            db = dz\n",
        "\n",
        "            # Update parameters with Stochastic Gradient Descent\n",
        "            W = W - learning_rate * dW\n",
        "            b = b - learning_rate * db\n",
        "        \n",
        "        if t % 100 == 0:\n",
        "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
        "            pred = predict(X, Y, W, b, word_to_vec_map) #predict is defined in emo_utils.py\n",
        "\n",
        "    return pred, W, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4Py5-saWSLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
        "print(X_train[0])\n",
        "print(type(X_train))\n",
        "Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])\n",
        "print(Y.shape)\n",
        "\n",
        "X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',\n",
        " 'Lets go party and drinks','Congrats on the new job','Congratulations',\n",
        " 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',\n",
        " 'You totally deserve this prize', 'Let us go play football',\n",
        " 'Are you down for football this afternoon', 'Work hard play harder',\n",
        " 'It is suprising how people can be dumb sometimes',\n",
        " 'I am very disappointed','It is the best day in my life',\n",
        " 'I think I will end up alone','My life is so boring','Good job',\n",
        " 'Great so awesome'])\n",
        "\n",
        "print(X.shape)\n",
        "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
        "print(type(X_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5R872p5WWi0",
        "colab_type": "text"
      },
      "source": [
        "Run the next cell to train your model and learn the softmax parameters (W,b). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLNmZKRXWXCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-8JWBmTWad3",
        "colab_type": "text"
      },
      "source": [
        "**Expected Output** (on a subset of iterations):\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **Epoch: 0**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 1.95204988128\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.348484848485\n",
        "        </td>\n",
        "    </tr>\n",
        "<tr>\n",
        "        <td>\n",
        "            **Epoch: 100**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 0.0797181872601\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.931818181818\n",
        "        </td>\n",
        "    </tr>\n",
        "<tr>\n",
        "        <td>\n",
        "            **Epoch: 200**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 0.0445636924368\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.954545454545\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **Epoch: 300**\n",
        "        </td>\n",
        "        <td>\n",
        "           cost = 0.0343226737879\n",
        "        </td>\n",
        "        <td>\n",
        "           Accuracy: 0.969696969697\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLi748TaWiCA",
        "colab_type": "text"
      },
      "source": [
        "Great! Your model has pretty high accuracy on the training set. Lets now see how it does on the test set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM68BrKHWk7p",
        "colab_type": "text"
      },
      "source": [
        "### 1.4 - Examining test set performance "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvBcql87WnvZ",
        "colab_type": "text"
      },
      "source": [
        "* Note that the `predict` function used here is defined in emo_util.spy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okFsCrmsWprf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Training set:\")\n",
        "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
        "print('Test set:')\n",
        "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5LpawasWrgY",
        "colab_type": "text"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **Train set accuracy**\n",
        "        </td>\n",
        "        <td>\n",
        "           97.7\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "            **Test set accuracy**\n",
        "        </td>\n",
        "        <td>\n",
        "           85.7\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBDTN7QjW1NW",
        "colab_type": "text"
      },
      "source": [
        "* Random guessing would have had 20% accuracy given that there are 5 classes. (1/5 = 20%).\n",
        "* This is pretty good performance after training on only 127 examples. \n",
        "\n",
        "\n",
        "#### The model matches emojis to relevant words\n",
        "In the training set, the algorithm saw the sentence \n",
        ">\"*I love you*\" \n",
        "\n",
        "with the label ‚ù§Ô∏è. \n",
        "* You can check that the word \"adore\" does not appear in the training set. \n",
        "* Nonetheless, lets see what happens if you write \"*I adore you*.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-vJuE9hW4bG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
        "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
        "\n",
        "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
        "print_predictions(X_my_sentences, pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5egFTHL4W_7X",
        "colab_type": "text"
      },
      "source": [
        "Amazing! \n",
        "* Because *adore* has a similar embedding as *love*, the algorithm has generalized correctly even to a word it has never seen before. \n",
        "* Words such as *heart*, *dear*, *beloved* or *adore* have embedding vectors similar to *love*. \n",
        "    * Feel free to modify the inputs above and try out a variety of input sentences. \n",
        "    * How well does it work?\n",
        "\n",
        "#### Word ordering isn't considered in this model\n",
        "* Note that the model doesn't get the following sentence correct:\n",
        ">\"not feeling happy\" \n",
        "\n",
        "* This algorithm ignores word ordering, so is not good at understanding phrases like \"not happy.\" \n",
        "\n",
        "#### Confusion matrix\n",
        "* Printing the confusion matrix can also help understand which classes are more difficult for your model. \n",
        "* A confusion matrix shows how often an example whose label is one class (\"actual\" class) is mislabeled by the algorithm with a different class (\"predicted\" class)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgVtsaQBXDcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(Y_test.shape)\n",
        "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
        "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
        "plot_confusion_matrix(Y_test, pred_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcVpFzsLXHy0",
        "colab_type": "text"
      },
      "source": [
        "## What you should remember from this section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgfzY5_mXIca",
        "colab_type": "text"
      },
      "source": [
        "- Even with a 127 training examples, you can get a reasonably good model for Emojifying. \n",
        "    - This is due to the generalization power word vectors gives you. \n",
        "- Emojify-V1 will perform poorly on sentences such as *\"This movie is not good and not enjoyable\"* \n",
        "    - It doesn't understand combinations of words.\n",
        "    - It just averages all the words' embedding vectors together, without considering the ordering of words. \n",
        "    \n",
        "**You will build a better algorithm in the next section!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUrk5_BMXMIi",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Emojifier-V2: Using LSTMs in Keras: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acf7ykPCXO1w",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFymrDnGW7hX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}