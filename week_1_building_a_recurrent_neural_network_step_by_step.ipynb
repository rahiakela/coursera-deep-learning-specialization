{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week-1-building_a_recurrent_neural_network_step_by_step.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/coursera-deep-learning-specialization/blob/course-5-sequence-models-recurrent-neural-networks/week_1_building_a_recurrent_neural_network_step_by_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7xKzEWrPXNq",
        "colab_type": "text"
      },
      "source": [
        "# Building your Recurrent Neural Network - Step by Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XlRROz0PX1o",
        "colab_type": "text"
      },
      "source": [
        "Welcome to Course 5's first assignment! In this assignment, you will implement key components of a Recurrent Neural Network in numpy.\n",
        "\n",
        "Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have \"memory\". They can read inputs $x^{\\langle t \\rangle}$ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a unidirectional RNN to take information from the past to process later inputs. A bidirectional RNN can take context from both the past and the future. \n",
        "\n",
        "**Notation**:\n",
        "- Superscript $[l]$ denotes an object associated with the $l^{th}$ layer. \n",
        "\n",
        "- Superscript $(i)$ denotes an object associated with the $i^{th}$ example. \n",
        "\n",
        "- Superscript $\\langle t \\rangle$ denotes an object at the $t^{th}$ time-step. \n",
        "    \n",
        "- **Sub**script $i$ denotes the $i^{th}$ entry of a vector.\n",
        "\n",
        "Example:  \n",
        "- $a^{(2)[3]<4>}_5$ denotes the activation of the 2nd training example (2), 3rd layer [3], 4th time step <4>, and 5th entry in the vector.\n",
        "\n",
        "#### Pre-requisites\n",
        "* We assume that you are already familiar with `numpy`.  \n",
        "* To refresh your knowledge of numpy, you can review course 1 of this specialization \"Neural Networks and Deep Learning\".  \n",
        "    * Specifically, review the week 2 assignment [\"Python Basics with numpy (optional)\"](https://www.coursera.org/learn/neural-networks-deep-learning/item/Zh0CU).\n",
        "    \n",
        "    \n",
        "#### Be careful when modifying the starter code\n",
        "* When working on graded functions, please remember to only modify the code that is between the\n",
        "```Python\n",
        "#### START CODE HERE\n",
        "```\n",
        "and\n",
        "```Python\n",
        "#### END CODE HERE\n",
        "```\n",
        "* In particular, Be careful to not modify the first line of graded routines. These start with:\n",
        "```Python\n",
        "# GRADED FUNCTION: routine_name\n",
        "```\n",
        "* The automatic grader (autograder) needs these to locate the function.\n",
        "* Even a change in spacing will cause issues with the autograder. \n",
        "* It will return 'failed' if these are modified or missing.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5JWdug8PksF",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Let's first import all the packages that you will need during this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyUTQNvQPmNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from rnn_utils import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nydS-QmZUTtm",
        "colab_type": "text"
      },
      "source": [
        "## 1 - Forward propagation for the basic Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0up3lanUUPJ",
        "colab_type": "text"
      },
      "source": [
        "Later this week, you will generate music using an RNN. The basic RNN that you will implement has the structure below. In this example, $T_x = T_y$. \n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-specialization/RNN.png?raw=1' width='800'/>\n",
        "<caption><center> **Figure 1**: Basic RNN model </center></caption>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9qP7CGeWxbY",
        "colab_type": "text"
      },
      "source": [
        "### Dimensions of input $x$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X4T-nE3WyP8",
        "colab_type": "text"
      },
      "source": [
        "#### Input with $n_x$ number of units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mbm4bmgW0pl",
        "colab_type": "text"
      },
      "source": [
        "* For a single input example, $x^{(i)}$ is a one-dimensional input vector.\n",
        "* Using language as an example, a language with a 5000 word vocabulary could be one-hot encoded into a vector that has 5000 units.  So $x^{(i)}$ would have the shape (5000,).  \n",
        "* We'll use the notation $n_x$ to denote the number of units in a single training example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ9kFT3QW6tY",
        "colab_type": "text"
      },
      "source": [
        "#### Batches of size $m$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cSn6OtfW7V7",
        "colab_type": "text"
      },
      "source": [
        "* Let's say we have mini-batches, each with 20 training examples.  \n",
        "* To benefit from vectorization, we'll stack 20 columns of $x^{(i)}$ examples into a 2D array (a matrix).\n",
        "* For example, this tensor has the shape (5000,20). \n",
        "* We'll use $m$ to denote the number of training examples.  \n",
        "* So the shape of a mini-batch is $(n_x,m)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxP4PSqqW_ph",
        "colab_type": "text"
      },
      "source": [
        "#### Time steps of size $T_{x}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vSA1VCXAL-",
        "colab_type": "text"
      },
      "source": [
        "* A recurrent neural network has multiple time steps, which we'll index with $t$.\n",
        "* In the lessons, we saw a single training example $x^{(i)}$ (a vector) pass through multiple time steps $T_x$.  For example, if there are 10 time steps, $T_{x} = 10$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ay7jbLWXCx3",
        "colab_type": "text"
      },
      "source": [
        "#### 3D Tensor of shape $(n_{x},m,T_{x})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEgG_Hc1XFOQ",
        "colab_type": "text"
      },
      "source": [
        "* The 3-dimensional tensor $x$ of shape $(n_x,m,T_x)$ represents the input $x$ that is fed into the RNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LC6XpyZXHKB",
        "colab_type": "text"
      },
      "source": [
        "#### Taking a 2D slice for each time step: $x^{\\langle t \\rangle}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD0IPRUxXJO9",
        "colab_type": "text"
      },
      "source": [
        "* At each time step, we'll use a mini-batches of training examples (not just a single example).\n",
        "* So, for each time step $t$, we'll use a 2D slice of shape $(n_x,m)$.\n",
        "* We're referring to this 2D slice as $x^{\\langle t \\rangle}$.  The variable name in the code is `xt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDuu_ii8XLVJ",
        "colab_type": "text"
      },
      "source": [
        "### Definition of hidden state $a$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXaXC9qYXTuF",
        "colab_type": "text"
      },
      "source": [
        "* The activation $a^{\\langle t \\rangle}$ that is passed to the RNN from one time step to another is called a \"hidden state.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGV2uwqIXVw1",
        "colab_type": "text"
      },
      "source": [
        "### Dimensions of hidden state $a$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjyF7m43XX8C",
        "colab_type": "text"
      },
      "source": [
        "* Similar to the input tensor $x$, the hidden state for a single training example is a vector of length $n_{a}$.\n",
        "* If we include a mini-batch of $m$ training examples, the shape of a mini-batch is $(n_{a},m)$.\n",
        "* When we include the time step dimension, the shape of the hidden state is $(n_{a}, m, T_x)$\n",
        "* We will loop through the time steps with index $t$, and work with a 2D slice of the 3D tensor.  \n",
        "* We'll refer to this 2D slice as $a^{\\langle t \\rangle}$. \n",
        "* In the code, the variable names we use are either `a_prev` or `a_next`, depending on the function that's being implemented.\n",
        "* The shape of this 2D slice is $(n_{a}, m)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJkYPbnjXa--",
        "colab_type": "text"
      },
      "source": [
        "### Dimensions of prediction $\\hat{y}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvlt5mv7Xf9Z",
        "colab_type": "text"
      },
      "source": [
        "* Similar to the inputs and hidden states, $\\hat{y}$ is a 3D tensor of shape $(n_{y}, m, T_{y})$.\n",
        "    * $n_{y}$: number of units in the vector representing the prediction.\n",
        "    * $m$: number of examples in a mini-batch.\n",
        "    * $T_{y}$: number of time steps in the prediction.\n",
        "* For a single time step $t$, a 2D slice $\\hat{y}^{\\langle t \\rangle}$ has shape $(n_{y}, m)$.\n",
        "* In the code, the variable names are:\n",
        "    - `y_pred`: $\\hat{y}$ \n",
        "    - `yt_pred`: $\\hat{y}^{\\langle t \\rangle}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU1Mbr15XlZ9",
        "colab_type": "text"
      },
      "source": [
        "Here's how you can implement an RNN: \n",
        "\n",
        "**Steps**:\n",
        "1. Implement the calculations needed for one time-step of the RNN.\n",
        "2. Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTa0iu3bXmaJ",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 - RNN cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U1qoZXTXpK9",
        "colab_type": "text"
      },
      "source": [
        "A recurrent neural network can be seen as the repeated use of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell. \n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-specialization/rnn_step_forward_figure2_v3a.png?raw=1' width='800'/>\n",
        "<caption><center> **Figure 2**: Basic RNN cell. Takes as input $x^{\\langle t \\rangle}$ (current input) and $a^{\\langle t - 1\\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\\langle t \\rangle}$ which is given to the next RNN cell and also used to predict $\\hat{y}^{\\langle t \\rangle}$ </center></caption>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsYPmYAndgtZ",
        "colab_type": "text"
      },
      "source": [
        "#### rnn cell versus rnn_cell_forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYc4G1SFdiqf",
        "colab_type": "text"
      },
      "source": [
        "* Note that an RNN cell outputs the hidden state $a^{\\langle t \\rangle}$.  \n",
        "    * The rnn cell is shown in the figure as the inner box which has solid lines.  \n",
        "* The function that we will implement, `rnn_cell_forward`, also calculates the prediction $\\hat{y}^{\\langle t \\rangle}$\n",
        "    * The rnn_cell_forward is shown in the figure as the outer box that has dashed lines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ae7aKbqdljv",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**: Implement the RNN-cell described in Figure (2).\n",
        "\n",
        "**Instructions**:\n",
        "1. Compute the hidden state with tanh activation: $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\n",
        "2. Using your new hidden state $a^{\\langle t \\rangle}$, compute the prediction $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$. We provided the function `softmax`.\n",
        "3. Store $(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$ in a `cache`.\n",
        "4. Return $a^{\\langle t \\rangle}$ , $\\hat{y}^{\\langle t \\rangle}$ and `cache`\n",
        "\n",
        "#### Additional Hints\n",
        "* [numpy.tanh](https://www.google.com/search?q=numpy+tanh&rlz=1C5CHFA_enUS854US855&oq=numpy+tanh&aqs=chrome..69i57j0l5.1340j0j7&sourceid=chrome&ie=UTF-8)\n",
        "* We've created a `softmax` function that you can use.  It is located in the file 'rnn_utils.py' and has been imported.\n",
        "* For matrix multiplication, use [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPHT9R0LUQqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCTION: rnn_cell_forward\n",
        "def rnn_cell_forward(xt, a_prev, parameters):\n",
        "  '''\n",
        "  Implements a single forward step of the RNN-cell as described in Figure (2)\n",
        "\n",
        "  Arguments:\n",
        "  xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
        "  a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "  parameters -- python dictionary containing:\n",
        "                      Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                      Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                      Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                      ba --  Bias, numpy array of shape (n_a, 1)\n",
        "                      by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "  Returns:\n",
        "  a_next -- next hidden state, of shape (n_a, m)\n",
        "  yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
        "  cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
        "  '''\n",
        "\n",
        "  # Retrieve parameters from \"parameters\"\n",
        "  Wax = parameters[\"Wax\"]\n",
        "  Waa = parameters[\"Waa\"]\n",
        "  Wya = parameters[\"Wya\"]\n",
        "  ba = parameters[\"ba\"]\n",
        "  by = parameters[\"by\"]\n",
        "\n",
        "  ### START CODE HERE ### (≈2 lines)\n",
        "  # compute next activation state using the formula given above\n",
        "  a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba)\n",
        "  # compute output of the current cell using the formula given above\n",
        "  yt_pred = softmax(np.dot(Wya, a_next) + by)   \n",
        "  ### END CODE HERE ###\n",
        "  \n",
        "  # store values you need for backward propagation in cache\n",
        "  cache = (a_next, a_prev, xt, parameters)\n",
        "  \n",
        "  return a_next, yt_pred, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqtpmVPAg2cI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "747db36e-cfaa-4d50-8458-baf419a1486b"
      },
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "xt_tmp = np.random.randn(3, 10)\n",
        "a_prev_tmp = np.random.randn(5, 10)\n",
        "\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
        "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
        "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
        "parameters_tmp['ba'] = np.random.randn(5, 1)\n",
        "parameters_tmp['by'] = np.random.randn(2, 1)\n",
        "\n",
        "a_next_tmp, yt_pred_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)\n",
        "print(f'a_next[4] = \\n {a_next_tmp[4]}')\n",
        "print(f'a_next.shape = \\n {a_next_tmp.shape}')\n",
        "print(f'yt_pred[1] =\\n {yt_pred_tmp[1]}')\n",
        "print(f'yt_pred.shape = \\n {yt_pred_tmp.shape}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_next[4] = \n",
            " [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
            " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
            "a_next.shape = \n",
            " (5, 10)\n",
            "yt_pred[1] =\n",
            " [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n",
            " 0.36920224 0.9966312  0.9982559  0.17746526]\n",
            "yt_pred.shape = \n",
            " (2, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD1ClyWeiOi9",
        "colab_type": "text"
      },
      "source": [
        "**Expected Output**: \n",
        "```Python\n",
        "a_next[4] = \n",
        " [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
        " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
        "a_next.shape = \n",
        " (5, 10)\n",
        "yt_pred[1] =\n",
        " [ 0.9888161   0.01682021  0.21140899  0.36817467  0.98988387  0.88945212\n",
        "  0.36920224  0.9966312   0.9982559   0.17746526]\n",
        "yt_pred.shape = \n",
        " (2, 10)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN3EVkdTiPaP",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 - RNN forward pass "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wioRRKtoiSz0",
        "colab_type": "text"
      },
      "source": [
        "- A recurrent neural network (RNN) is a repetition of the RNN cell that you've just built. \n",
        "    - If your input sequence of data is 10 time steps long, then you will re-use the RNN cell 10 times. \n",
        "- Each cell takes two inputs at each time step:\n",
        "    - $a^{\\langle t-1 \\rangle}$: The hidden state from the previous cell.\n",
        "    - $x^{\\langle t \\rangle}$: The current time-step's input data.\n",
        "- It has two outputs at each time step:\n",
        "    - A hidden state ($a^{\\langle t \\rangle}$)\n",
        "    - A prediction ($y^{\\langle t \\rangle}$)\n",
        "- The weights and biases $(W_{aa}, b_{a}, W_{ax}, b_{x})$ are re-used each time step. \n",
        "    - They are maintained between calls to rnn_cell_forward in the 'parameters' dictionary.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-specialization/rnn_forward_sequence_figure3_v3a.png?raw=1' width='800'/>\n",
        "<caption><center> **Figure 3**: Basic RNN. The input sequence $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$  is carried over $T_x$ time steps. The network outputs $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. </center></caption>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0jr4cqJiVqX",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**: Code the forward propagation of the RNN described in Figure (3).\n",
        "\n",
        "**Instructions**:\n",
        "* Create a 3D array of zeros, $a$ of shape $(n_{a}, m, T_{x})$ that will store all the hidden states computed by the RNN.\n",
        "* Create a 3D array of zeros, $\\hat{y}$, of shape $(n_{y}, m, T_{x})$ that will store the predictions.  \n",
        "    - Note that in this case, $T_{y} = T_{x}$ (the prediction and input have the same number of time steps).\n",
        "* Initialize the 2D hidden state `a_next` by setting it equal to the initial hidden state, $a_{0}$.\n",
        "* At each time step $t$:\n",
        "    - Get $x^{\\langle t \\rangle}$, which is a 2D slice of $x$ for a single time step $t$.\n",
        "        - $x^{\\langle t \\rangle}$ has shape $(n_{x}, m)$\n",
        "        - $x$ has shape $(n_{x}, m, T_{x})$\n",
        "    - Update the 2D hidden state $a^{\\langle t \\rangle}$ (variable name `a_next`), the prediction $\\hat{y}^{\\langle t \\rangle}$ and the cache by running `rnn_cell_forward`.\n",
        "        - $a^{\\langle t \\rangle}$ has shape $(n_{a}, m)$\n",
        "    - Store the 2D hidden state in the 3D tensor $a$, at the $t^{th}$ position.\n",
        "        - $a$ has shape $(n_{a}, m, T_{x})$\n",
        "    - Store the 2D $\\hat{y}^{\\langle t \\rangle}$ prediction (variable name `yt_pred`) in the 3D tensor $\\hat{y}_{pred}$ at the $t^{th}$ position.\n",
        "        - $\\hat{y}^{\\langle t \\rangle}$ has shape $(n_{y}, m)$\n",
        "        - $\\hat{y}$ has shape $(n_{y}, m, T_x)$\n",
        "    - Append the cache to the list of caches.\n",
        "* Return the 3D tensor $a$ and $\\hat{y}$, as well as the list of caches.\n",
        "\n",
        "#### Additional Hints\n",
        "- [np.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html)\n",
        "- If you have a 3 dimensional numpy array and are indexing by its third dimension, you can use array slicing like this: `var_name[:,:,i]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g02GCaXRiJ6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNCTION: rnn_forward\n",
        "def rnn_forward(x, a0, parameters):\n",
        "  '''\n",
        "  Implement the forward propagation of the recurrent neural network described in Figure (3).\n",
        "\n",
        "  Arguments:\n",
        "  x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
        "  a0 -- Initial hidden state, of shape (n_a, m)\n",
        "  parameters -- python dictionary containing:\n",
        "                      Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                      Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                      Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                      ba --  Bias numpy array of shape (n_a, 1)\n",
        "                      by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "\n",
        "  Returns:\n",
        "  a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
        "  y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
        "  caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
        "  '''\n",
        "\n",
        "  # Initialize \"caches\" which will contain the list of all caches\n",
        "  caches = []\n",
        "\n",
        "  # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n",
        "  n_x, m, T_x = x.shape\n",
        "  n_y, n_a = parameters['Wya'].shape\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "\n",
        "  # initialize \"a\" and \"y_pred\" with zeros (≈2 lines)\n",
        "  a = np.zeros((n_a, m, T_x))\n",
        "  y_pred = np.zeros((n_y, m, T_x))\n",
        "\n",
        "  # Initialize a_next (≈1 line)\n",
        "  a_next = a0\n",
        "\n",
        "  # loop over all time-steps of the input 'x' (1 line)\n",
        "  for t in range(T_x):\n",
        "    # Update next hidden state, compute the prediction, get the cache (≈2 lines)\n",
        "    xt = x[:, :, t]\n",
        "    a_next, yt_pred, cache = rnn_cell_forward(xt, a_next, parameters)\n",
        "    # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
        "    a[:, :, t] = a_next\n",
        "    # Save the value of the prediction in y (≈1 line)\n",
        "    y_pred[:, :, t] = yt_pred\n",
        "    # Append \"cache\" to \"caches\" (≈1 line)\n",
        "    caches.append(cache)\n",
        "\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  # store values needed for backward propagation in cache\n",
        "  caches = (caches, x)\n",
        "\n",
        "  return a, y_pred, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS8dvSNj6DH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "785a430f-32d3-4a93-b82a-2bd690a749f0"
      },
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "x_tmp = np.random.randn(3, 10, 4)\n",
        "a0_tmp = np.random.randn(5, 10)\n",
        "\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Waa'] = np.random.randn(5, 5)\n",
        "parameters_tmp['Wax'] = np.random.randn(5, 3)\n",
        "parameters_tmp['Wya'] = np.random.randn(2, 5)\n",
        "parameters_tmp['ba'] = np.random.randn(5, 1)\n",
        "parameters_tmp['by'] = np.random.randn(2, 1)\n",
        "\n",
        "a_tmp, y_pred_tmp, caches_tmp = rnn_forward(x_tmp, a0_tmp, parameters_tmp)\n",
        "print(f'a[4][1] = \\n {a_tmp[4][1]}')\n",
        "print(f'a.shape = \\n {a_tmp.shape}')\n",
        "print(f'y_pred[1][3] =\\n {y_pred_tmp[1][3]}')\n",
        "print(f'y_pred.shape = \\n {y_pred_tmp.shape}')\n",
        "print(f'caches[1][1][3] =\\n {caches_tmp[1][1][3]}')\n",
        "print(f'len(caches) = \\n {len(caches_tmp)}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a[4][1] = \n",
            " [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
            "a.shape = \n",
            " (5, 10, 4)\n",
            "y_pred[1][3] =\n",
            " [0.79560373 0.86224861 0.11118257 0.81515947]\n",
            "y_pred.shape = \n",
            " (2, 10, 4)\n",
            "caches[1][1][3] =\n",
            " [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
            "len(caches) = \n",
            " 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PYHeCbg9RAG",
        "colab_type": "text"
      },
      "source": [
        "**Expected Output**:\n",
        "\n",
        "```Python\n",
        "a[4][1] = \n",
        " [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
        "a.shape = \n",
        " (5, 10, 4)\n",
        "y_pred[1][3] =\n",
        " [ 0.79560373  0.86224861  0.11118257  0.81515947]\n",
        "y_pred.shape = \n",
        " (2, 10, 4)\n",
        "caches[1][1][3] =\n",
        " [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
        "len(caches) = \n",
        " 2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go2o-nl59TAh",
        "colab_type": "text"
      },
      "source": [
        "Congratulations! You've successfully built the forward propagation of a recurrent neural network from scratch. \n",
        "\n",
        "#### Situations when this RNN will perform better:\n",
        "- This will work well enough for some applications, but it suffers from the vanishing gradient problems. \n",
        "- The RNN works best when each output $\\hat{y}^{\\langle t \\rangle}$ can be estimated using \"local\" context.  \n",
        "- \"Local\" context refers to information that is close to the prediction's time step $t$.\n",
        "- More formally, local context refers to inputs $x^{\\langle t' \\rangle}$ and predictions $\\hat{y}^{\\langle t \\rangle}$ where $t'$ is close to $t$.\n",
        "\n",
        "In the next part, you will build a more complex LSTM model, which is better at addressing vanishing gradients. The LSTM will be better able to remember a piece of information and keep it saved for many timesteps. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbQnK4vI9Yoh",
        "colab_type": "text"
      },
      "source": [
        "## 2 - Long Short-Term Memory (LSTM) network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvkmOujs9a_8",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UP8zhAx8AQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}